# ✅ Deep Learning Knowledge Checklist


---

## 1️⃣ Transformer & Attention: Core Mechanics

- [☑️] Eself-attention end-to-end
- [☑️] Distinguish **self-attention** vs **cross-attention**
- [☑️] Multi-head Attention
- [☑️] Positional encoding
- [☑️] Time complexity of transformer

---

## 2️⃣ Transformer Variants & Design Choices

- [ ] Encoder-only vs decoder-only vs encoder–decoder (when to use each)
- [ ] Causal masking vs bidirectional attention (why it matters)
- [ ] KV-cache: what it stores and how it speeds up decoding
- [ ] Sliding-window / sparse attention (why & when)


---

## 3️⃣ Layer Normalization & Batch Normalization

- [ ] Layer Normalization
- [ ] Batch Normalization
- [ ] LN vs. BN

