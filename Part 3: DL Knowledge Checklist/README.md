# ✅ Deep Learning Knowledge Checklist


---

## 1️⃣ Transformer & Attention: Core Mechanics

- [ ] Eself-attention end-to-end
- [ ] Distinguish **self-attention** vs **cross-attention**
- [ ] Multi-head Attention
- [ ] Ppositional encoding
- [ ] Describe attention compute/memory complexity and why it becomes a bottleneck

---

## 2️⃣ Transformer Variants & Design Choices

- [ ] Encoder-only vs decoder-only vs encoder–decoder (when to use each)
- [ ] Causal masking vs bidirectional attention (why it matters)
- [ ] KV-cache: what it stores and how it speeds up decoding
- [ ] Sliding-window / sparse attention (why & when)
