# ✅ Deep Learning Knowledge Checklist


---

## 1️⃣ Transformer & Attention: Core Mechanics

- [☑️] Eself-attention end-to-end
- [☑️] Distinguish **self-attention** vs **cross-attention**
- [☑️] Multi-head Attention
- [☑️] Positional encoding
- [☑️] Time complexity of transformer

---

## 2️⃣ Transformer Variants & Design Choices

- [☑️] Encoder-only vs decoder-only vs encoder–decoder (when to use each)
- [☑️] Causal masking vs bidirectional attention (why it matters)

---

## 3️⃣ Layer Normalization & Batch Normalization

- [☑️] Layer Normalization
- [☑️] Batch Normalization
- [☑️] LN vs. BN


---

## 4️⃣ Advanced Attention Variants

- [☑️] Sparse Attention: Sliding window Attention, Dilated Attention, Block Sparse Attention
- [☑️] FlashAttention
- [☑️] Multi-query/Grouped Attention
- [☑️] Relative positional attention
- [] Rotary (RoPE) Attention

