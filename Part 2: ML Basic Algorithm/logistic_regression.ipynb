{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2dbaee9",
   "metadata": {},
   "source": [
    "Reference: https://tech.meituan.com/2015/05/08/intro-to-logistic-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e55b19",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b931c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n, k = 100, 8\n",
    "\n",
    "X = np.random.randn(n, k) # 100*8\n",
    "y = (np.random.rand(n)>0.5).astype(float)\n",
    "\n",
    "w = np.random.randn(k) # 8*1\n",
    "\n",
    "alpha = 1e-2\n",
    "max_itr = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cbbd8",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24fb4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685bfc3",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af222d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w):\n",
    "    z = X@w # X: 100*8 w: 8*1\n",
    "    y_hat = sigmoid(z)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde92ff",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa4db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat, y):\n",
    "    eps = 1e-8 # avoid log(0)\n",
    "    L = -np.sum(\n",
    "        y*np.log(y_hat+eps)\n",
    "        +(1-y)*np.log(1-y_hat+eps)\n",
    "    )\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4ab5a",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ada556e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y_hat, y):\n",
    "    grad_w = X.T@(y_hat-y)\n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e234e77",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e89b6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, w, alpha, max_itr):\n",
    "    for i in range(max_itr):\n",
    "        # Forward\n",
    "        y_hat = forward(X, w)\n",
    "\n",
    "        # Loss\n",
    "        L = loss(y_hat, y)\n",
    "\n",
    "        # Backward\n",
    "        grad_w = backward(X, y_hat, y)\n",
    "\n",
    "        # Update\n",
    "        w -=alpha*grad_w\n",
    "\n",
    "        if i%10==0:\n",
    "            print(f\"iter {i}, loss = {L:.4f}, w = {w}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34706d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c99a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss = 137.6925, w = [-0.20986707  0.46678602  1.02849606  0.12897207  0.93640699 -0.34632775\n",
      " -1.39946473  1.03463403]\n",
      "iter 10, loss = 68.6264, w = [-0.07387965  0.10775646 -0.18564655  0.1230656  -0.04278575  0.10566059\n",
      " -0.34125848  0.23048406]\n",
      "iter 20, loss = 66.5939, w = [ 0.02593231  0.03125915 -0.29354913  0.01756559 -0.17305792  0.16484553\n",
      " -0.18472233  0.07250168]\n",
      "iter 30, loss = 66.5653, w = [ 0.04344187  0.01812966 -0.30612263  0.00544818 -0.18887091  0.17251562\n",
      " -0.17188002  0.05500966]\n",
      "iter 40, loss = 66.5648, w = [ 0.04634987  0.01580502 -0.30769378  0.00414394 -0.19076198  0.17331815\n",
      " -0.1709228   0.05281693]\n",
      "iter 50, loss = 66.5648, w = [ 0.04683182  0.01538992 -0.30789426  0.00400333 -0.19099457  0.17336646\n",
      " -0.1708703   0.05251831]\n",
      "iter 60, loss = 66.5648, w = [ 0.04691247  0.0153154  -0.30792044  0.00398822 -0.1910254   0.17336023\n",
      " -0.17087185  0.05247601]\n",
      "iter 70, loss = 66.5648, w = [ 0.04692616  0.01530193 -0.30792392  0.00398662 -0.19102991  0.17335682\n",
      " -0.17087314  0.05246989]\n",
      "iter 80, loss = 66.5648, w = [ 0.04692852  0.01529948 -0.30792439  0.00398645 -0.19103063  0.17335586\n",
      " -0.17087346  0.05246899]\n",
      "iter 90, loss = 66.5648, w = [ 0.04692893  0.01529903 -0.30792445  0.00398644 -0.19103075  0.17335563\n",
      " -0.17087353  0.05246885]\n",
      "iter 100, loss = 66.5648, w = [ 0.046929    0.01529895 -0.30792446  0.00398644 -0.19103078  0.17335558\n",
      " -0.17087354  0.05246883]\n",
      "iter 110, loss = 66.5648, w = [ 0.04692902  0.01529894 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 120, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 130, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 140, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 150, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 160, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 170, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 180, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 190, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 200, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 210, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 220, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 230, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 240, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 250, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 260, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 270, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 280, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 290, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 300, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 310, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 320, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 330, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 340, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 350, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 360, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 370, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 380, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 390, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 400, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 410, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 420, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 430, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 440, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 450, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 460, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 470, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 480, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 490, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 500, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 510, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 520, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 530, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 540, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 550, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 560, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 570, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 580, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 590, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 600, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 610, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 620, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 630, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 640, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 650, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 660, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 670, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 680, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 690, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 700, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 710, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 720, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 730, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 740, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 750, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 760, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 770, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 780, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 790, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 800, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 810, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 820, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 830, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 840, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 850, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 860, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 870, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 880, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 890, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 900, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 910, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 920, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 930, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 940, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 950, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 960, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 970, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 980, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n",
      "iter 990, loss = 66.5648, w = [ 0.04692902  0.01529893 -0.30792446  0.00398644 -0.19103078  0.17335557\n",
      " -0.17087354  0.05246883]\n"
     ]
    }
   ],
   "source": [
    "w_final = fit(X, y, w, alpha, max_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ce88f",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8773c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, threshold=0.5):\n",
    "    probs = forward(X, w)\n",
    "    return (probs>=threshold).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prod_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
