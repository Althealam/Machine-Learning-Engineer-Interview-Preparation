{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfdc934",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d4e941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4]) # shape=(4, )\n",
    "y = np.array([3, 5, 7, 9]) # y=2x+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2938bf",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce424ad",
   "metadata": {},
   "source": [
    "### 1.1 Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee929b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    return w*x+b\n",
    "\n",
    "def gradients(x, y, w, b):\n",
    "    y_pred = predict(x, w, b)\n",
    "    dw = (y_pred-y)*x\n",
    "    db = (y_pred-y)\n",
    "    return dw, db\n",
    "\n",
    "def mse_loss(x, y, w, b):\n",
    "    prediction = predict(x, w, b)\n",
    "    return ((prediction-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aebcaf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, w: 1.75, b: 0.6000000000000001, MSE Loss: 1.1287500000000006\n",
      "Epoch 1, w: 2.0375, b: 0.7025000000000001, MSE Loss: 0.0432718749999998\n",
      "Epoch 2, w: 2.0837499999999998, b: 0.722875, MSE Loss: 0.013357640625000014\n",
      "Epoch 3, w: 2.09021875, b: 0.7296500000000001, MSE Loss: 0.01218159857421868\n",
      "Epoch 4, w: 2.0901421875, b: 0.7341303125000002, MSE Loss: 0.011798419380029259\n",
      "Epoch 5, w: 2.08900296875, b: 0.7381817343750001, MSE Loss: 0.011447252994228812\n",
      "Epoch 6, w: 2.08770530859375, b: 0.7421128187500001, MSE Loss: 0.011107082849989253\n",
      "Epoch 7, w: 2.0863981224609374, b: 0.7459752097265626, MSE Loss: 0.010777036118485434\n",
      "Epoch 8, w: 2.085105728183594, b: 0.749778158138672, MSE Loss: 0.010456797121681672\n",
      "Epoch 9, w: 2.0838318925112307, b: 0.7535239102789063, MSE Loss: 0.010146074018873748\n",
      "Epoch 10, w: 2.0825769955580813, b: 0.757213546123208, MSE Loss: 0.009844584034832125\n",
      "Epoch 11, w: 2.0813408623587186, b: 0.7608479426213669, MSE Loss: 0.00955205280768426\n",
      "Epoch 12, w: 2.0801232299343377, b: 0.7644279327695506, MSE Loss: 0.009268214128495184\n",
      "Epoch 13, w: 2.0789238242911967, b: 0.7679543320090111, MSE Loss: 0.008992809698721103\n",
      "Epoch 14, w: 2.077742373070546, b: 0.7714279427353108, MSE Loss: 0.008725588895143712\n",
      "Epoch 15, w: 2.0765786075838086, b: 0.7748495551941431, MSE Loss: 0.00846630854179895\n",
      "Epoch 16, w: 2.0754322630974165, b: 0.7782199477787767, MSE Loss: 0.008214732688682063\n",
      "Epoch 17, w: 2.07430307882966, b: 0.7815398872265449, MSE Loss: 0.007970632397028514\n",
      "Epoch 18, w: 2.0731907979007786, b: 0.7848101287964754, MSE Loss: 0.007733785530975434\n",
      "Epoch 19, w: 2.0720951672760757, b: 0.7880314164416332, MSE Loss: 0.007503976555413924\n",
      "Epoch 20, w: 2.0710159377086104, b: 0.7912044829784509, MSE Loss: 0.007280996339848065\n",
      "Epoch 21, w: 2.06995286368254, b: 0.7943300502534533, MSE Loss: 0.00706464196808205\n",
      "Epoch 22, w: 2.0689057033572715, b: 0.7974088293074729, MSE Loss: 0.0068547165535627445\n",
      "Epoch 23, w: 2.0678742185124497, b: 0.8004415205374077, MSE Loss: 0.0066510290602091755\n",
      "Epoch 24, w: 2.0668581744937606, b: 0.8034288138555545, MSE Loss: 0.006453394128566084\n",
      "Epoch 25, w: 2.0658573401595515, b: 0.8063713888465589, MSE Loss: 0.006261631907123566\n",
      "Epoch 26, w: 2.064871487828248, b: 0.8092699149220152, MSE Loss: 0.006075567888648342\n",
      "Epoch 27, w: 2.0639003932265583, b: 0.8121250514727516, MSE Loss: 0.005895032751379217\n",
      "Epoch 28, w: 2.0629438354384515, b: 0.8149374480188369, MSE Loss: 0.005719862204941142\n",
      "Epoch 29, w: 2.0620015968549037, b: 0.8177077443573404, MSE Loss: 0.0055498968408376585\n",
      "Epoch 30, w: 2.0610734631243908, b: 0.8204365707078805, MSE Loss: 0.005384981987386345\n",
      "Epoch 31, w: 2.0601592231041277, b: 0.8231245478559948, MSE Loss: 0.005224967568964513\n",
      "Epoch 32, w: 2.059258668812033, b: 0.8257722872943634, MSE Loss: 0.0050697079694376895\n",
      "Epoch 33, w: 2.0583715953794175, b: 0.8283803913619188, MSE Loss: 0.0049190618996460996\n",
      "Epoch 34, w: 2.057497801004375, b: 0.8309494533808726, MSE Loss: 0.00477289226882905\n",
      "Epoch 35, w: 2.0566370869058757, b: 0.8334800577916915, MSE Loss: 0.0046310660598694285\n",
      "Epoch 36, w: 2.055789257278546, b: 0.8359727802860535, MSE Loss: 0.004493454208245967\n",
      "Epoch 37, w: 2.0549541192481233, b: 0.8384281879378117, MSE Loss: 0.004359931484581941\n",
      "Epoch 38, w: 2.054131482827578, b: 0.8408468393319997, MSE Loss: 0.004230376380684039\n",
      "Epoch 39, w: 2.0533211608738946, b: 0.8432292846919053, MSE Loss: 0.004104670998967532\n",
      "Epoch 40, w: 2.0525229690454974, b: 0.8455760660042411, MSE Loss: 0.00398270094516772\n",
      "Epoch 41, w: 2.051736725760314, b: 0.8478877171424426, MSE Loss: 0.0038643552242383253\n",
      "Epoch 42, w: 2.0509622521544677, b: 0.8501647639881198, MSE Loss: 0.003749526139344382\n",
      "Epoch 43, w: 2.050199372041587, b: 0.8524077245506909, MSE Loss: 0.0036381091938559548\n",
      "Epoch 44, w: 2.049447911872724, b: 0.854617109085225, MSE Loss: 0.003530002996254253\n",
      "Epoch 45, w: 2.0487077006968746, b: 0.8567934202085216, MSE Loss: 0.003425109167863363\n",
      "Epoch 46, w: 2.0479785701220883, b: 0.8589371530134507, MSE Loss: 0.0033233322533238324\n",
      "Epoch 47, w: 2.0472603542771597, b: 0.8610487951815836, MSE Loss: 0.0032245796337266467\n",
      "Epoch 48, w: 2.0465528897738943, b: 0.8631288270941353, MSE Loss: 0.0031287614423280074\n",
      "Epoch 49, w: 2.0458560156699397, b: 0.8651777219412482, MSE Loss: 0.0030357904827690804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(2.0458560156699397), np.float64(0.8651777219412482))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_gd(X, y, lr=0.1, epochs=50):\n",
    "    w, b = 0.0, 0.0\n",
    "    N = len(X) # the number of dataset\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        dw_sum, db_sum = 0.0, 0.0\n",
    "        for i in range(N): # iterate all data points in the dataset\n",
    "            dw, db = gradients(X[i], y[i], w, b)\n",
    "            dw_sum+=dw\n",
    "            db_sum+=db\n",
    "\n",
    "        # average gradient\n",
    "        w-=lr*dw_sum/N\n",
    "        b-=lr*db_sum/N\n",
    "        mse = mse_loss(X, y, w, b)\n",
    "        print(f\"Epoch {ep}, w: {w}, b: {b}, MSE Loss: {mse}\")\n",
    "    return w, b\n",
    "\n",
    "batch_gd(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979407f",
   "metadata": {},
   "source": [
    "### 1.2 Mini-Batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15f51c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, w: 1.995, b: 0.7666666666666666, MSE Loss: 0.0604902777777779\n",
      "Epoch: 1, w: 2.05885, b: 0.8009, MSE Loss: 0.010493876250000022\n",
      "Epoch: 2, w: 2.058465, b: 0.8074165833333333, MSE Loss: 0.009845753010423591\n",
      "Epoch: 3, w: 2.0679317543055555, b: 0.8151881047222221, MSE Loss: 0.0013781564020913693\n",
      "Epoch: 4, w: 2.0446728078092593, b: 0.8124572222435186, MSE Loss: 0.001471603191045254\n",
      "Epoch: 5, w: 2.070642496538982, b: 0.827365902870662, MSE Loss: 0.005692526457903304\n",
      "Epoch: 6, w: 2.0508497560172603, b: 0.8257898866742223, MSE Loss: 0.0030548899049989272\n",
      "Epoch: 7, w: 2.0510340471803175, b: 0.8342463789877562, MSE Loss: 0.007316901062058425\n",
      "Epoch: 8, w: 2.048836201447731, b: 0.8381897440886142, MSE Loss: 0.0026191151125577296\n",
      "Epoch: 9, w: 2.0475956652118925, b: 0.8432051807821385, MSE Loss: 0.006526299170480884\n",
      "Epoch: 10, w: 2.05585770321142, b: 0.8502850673937674, MSE Loss: 0.0008814392384633646\n",
      "Epoch: 11, w: 2.043774493996368, b: 0.8507563895406949, MSE Loss: 0.002237337329026614\n",
      "Epoch: 12, w: 2.0524703253158005, b: 0.8571980731383353, MSE Loss: 0.0008234502764793363\n",
      "Epoch: 13, w: 2.034519390015007, b: 0.8550951850792747, MSE Loss: 0.0008780782257052406\n",
      "Epoch: 14, w: 2.0374223650964196, b: 0.8599195988409595, MSE Loss: 0.00043295700131353596\n",
      "Epoch: 15, w: 2.0408154975206547, b: 0.8656129533557456, MSE Loss: 0.004794698581979481\n",
      "Epoch: 16, w: 2.039549742395601, b: 0.8689671891808303, MSE Loss: 0.0017175352671382122\n",
      "Epoch: 17, w: 2.037751843464912, b: 0.8725545421728179, MSE Loss: 0.00162655572553731\n",
      "Epoch: 18, w: 2.031382096238092, b: 0.8740056545082788, MSE Loss: 0.0005072579198617995\n",
      "Epoch: 19, w: 2.036208230611341, b: 0.881811806055851, MSE Loss: 0.003715327972475996\n",
      "Epoch: 20, w: 2.0340833062016004, b: 0.8848654728607345, MSE Loss: 0.001327684898718471\n",
      "Epoch: 21, w: 2.043341729459878, b: 0.8932739923192082, MSE Loss: 0.0022096352432123363\n",
      "Epoch: 22, w: 2.0414552838272764, b: 0.8968302878314566, MSE Loss: 0.0021095518009640727\n",
      "Epoch: 23, w: 2.031172244991689, b: 0.8965714766983589, MSE Loss: 0.0010699524056526923\n",
      "Epoch: 24, w: 2.0368095471611456, b: 0.9013054757771103, MSE Loss: 0.00038323334629205403\n",
      "Epoch: 25, w: 2.028856094369849, b: 0.9016205178912964, MSE Loss: 0.0009721786040792938\n",
      "Epoch: 26, w: 2.028870947718938, b: 0.9049025454008559, MSE Loss: 0.0024007764601684796\n",
      "Epoch: 27, w: 2.027985988559284, b: 0.9078030055646531, MSE Loss: 0.002256497848514646\n",
      "Epoch: 28, w: 2.032521121931971, b: 0.9115244646721417, MSE Loss: 0.0003158539057244745\n",
      "Epoch: 29, w: 2.0229255282136966, b: 0.9108651609240886, MSE Loss: 0.0002105247091443174\n",
      "Epoch: 30, w: 2.0259004488075916, b: 0.9147347287316728, MSE Loss: 0.0019302051284025498\n",
      "Epoch: 31, w: 2.0250936591470556, b: 0.9168617829050633, MSE Loss: 0.000691427944365365\n",
      "Epoch: 32, w: 2.0313644150073915, b: 0.9228081640910694, MSE Loss: 0.0011546655229904804\n",
      "Epoch: 33, w: 2.0290685796044365, b: 0.9254039351860204, MSE Loss: 0.0011158778939372772\n",
      "Epoch: 34, w: 2.019340113832951, b: 0.9243285729654707, MSE Loss: 0.00015720681858837556\n",
      "Epoch: 35, w: 2.0285955217007374, b: 0.9299823985814158, MSE Loss: 0.0009401546393589661\n",
      "Epoch: 36, w: 2.026395674355325, b: 0.9322974639255472, MSE Loss: 0.0009190751332707872\n",
      "Epoch: 37, w: 2.024365505271811, b: 0.9337959903769195, MSE Loss: 0.0008989835439814062\n",
      "Epoch: 38, w: 2.025539028833336, b: 0.9366950578225182, MSE Loss: 0.0007878804577791066\n",
      "Epoch: 39, w: 2.024577511202227, b: 0.9388510415090856, MSE Loss: 0.0007406626298176649\n",
      "Epoch: 40, w: 2.0229744348887424, b: 0.9409797975348487, MSE Loss: 0.0006986843966887147\n",
      "Epoch: 41, w: 2.02123804015497, b: 0.9422961635360998, MSE Loss: 0.0006829389071086053\n",
      "Epoch: 42, w: 2.0212956193814344, b: 0.9448327759207503, MSE Loss: 0.0006116588115167584\n",
      "Epoch: 43, w: 2.0161728876642835, b: 0.9447640044613651, MSE Loss: 0.00030668485193220067\n",
      "Epoch: 44, w: 2.0142850581060117, b: 0.9456534443638013, MSE Loss: 6.992823729720435e-05\n",
      "Epoch: 45, w: 2.019029551148073, b: 0.9487662971871772, MSE Loss: 0.0005356939156711505\n",
      "Epoch: 46, w: 2.0184660778588634, b: 0.949883709886952, MSE Loss: 0.00010086016325297568\n",
      "Epoch: 47, w: 2.0184994947124473, b: 0.9520812037812625, MSE Loss: 0.00046147348503071877\n",
      "Epoch: 48, w: 2.0115268463328437, b: 0.9510483112662803, MSE Loss: 0.00010731000327789723\n",
      "Epoch: 49, w: 2.0168945694460385, b: 0.954208465744693, MSE Loss: 8.399543085589415e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(2.0168945694460385), np.float64(0.954208465744693))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mini_batch_sgd(X, y, lr=0.1, batch_size =2, epochs=50):\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        # shuffle\n",
    "        indices = np.random.permutation(N)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # 从shuffle后的数据中获取batch，并且遍历这个batch总共epochs次\n",
    "        for start in range(0, N, batch_size): # get the batch-size dataset\n",
    "            end = start+batch_size\n",
    "            dw_sum, db_sum = 0, 0\n",
    "            batch_X = X_shuffled[start:end+1]\n",
    "            batch_y = y_shuffled[start:end+1]\n",
    "            B = len(batch_X)\n",
    "\n",
    "            for i in range(B): # iterate all samples in this mini-batch\n",
    "                dw, db = gradients(batch_X[i], batch_y[i], w, b) # 如果不设置i的话 会导致计算gradients的时候维度不一致\n",
    "                dw_sum+=dw\n",
    "                db_sum+=db\n",
    "            \n",
    "            # 每次得到一个batch就更新一次参数\n",
    "            w-=lr*dw_sum/B\n",
    "            b-=lr*db_sum/B\n",
    "        \n",
    "        mse = mse_loss(batch_X, batch_y, w, b)\n",
    "        print(f\"Epoch: {ep}, w: {w}, b: {b}, MSE Loss: {mse}\")\n",
    "    return w, b\n",
    "\n",
    "mini_batch_sgd(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a171cda",
   "metadata": {},
   "source": [
    "### 1.3 SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56415b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, w: 1.9993999999999998, b: 1.0030999999999999, MSE: 3.0099999999977384e-06\n",
      "Epoch: 1, w: 1.9994429000000002, b: 1.00287835, MSE: 2.59495787250237e-06\n",
      "Epoch: 2, w: 1.9994827326500002, b: 1.0026725479749998, MSE: 2.2371449701169786e-06\n",
      "Epoch: 3, w: 1.9995197172655248, b: 1.0024814607947872, MSE: 1.928670083761853e-06\n",
      "Epoch: 4, w: 1.99955405748104, b: 1.00230403634796, MSE: 1.6627301054205377e-06\n",
      "Epoch: 5, w: 1.9995859423711462, b: 1.002139297749081, MSE: 1.4334599923297457e-06\n",
      "Epoch: 6, w: 1.999615547491609, b: 1.0019863379600216, MSE: 1.2358034192714807e-06\n",
      "Epoch: 7, w: 1.999643035845959, b: 1.0018443147958798, MSE: 1.0654012663448599e-06\n",
      "Epoch: 8, w: 1.9996685587829728, b: 1.0017124462879745, MSE: 9.184954828815488e-07\n",
      "Epoch: 9, w: 1.9996922568299904, b: 1.0015900063783845, MSE: 7.91846207362491e-07\n",
      "Epoch: 10, w: 1.9997142604666456, b: 1.0014763209223299, MSE: 6.826603154812293e-07\n",
      "Epoch: 11, w: 1.9997346908432805, b: 1.0013707639763834, MSE: 5.885298205654958e-07\n",
      "Epoch: 12, w: 1.9997536604479862, b: 1.001272754352072, MSE: 5.07378767801201e-07\n",
      "Epoch: 13, w: 1.9997712737259548, b: 1.0011817524158988, MSE: 4.3741745111013686e-07\n",
      "Epoch: 14, w: 1.9997876276545496, b: 1.001097257118162, MSE: 3.771029429670225e-07\n",
      "Epoch: 15, w: 1.999802812277249, b: 1.001018803234213, MSE: 3.2510506664204534e-07\n",
      "Epoch: 16, w: 1.9998169111994257, b: 1.000945958802967, MSE: 2.8027706048910556e-07\n",
      "Epoch: 17, w: 1.9998300020486666, b: 1.000878322748555, MSE: 2.4163028724133023e-07\n",
      "Epoch: 18, w: 1.999842156902187, b: 1.0008155226720332, MSE: 2.0831243060183675e-07\n",
      "Epoch: 19, w: 1.9998534426836811, b: 1.000757212800983, MSE: 1.7958869825000937e-07\n",
      "Epoch: 20, w: 1.9998639215317981, b: 1.0007030720857126, MSE: 1.5482561672324477e-07\n",
      "Epoch: 21, w: 1.9998736511422743, b: 1.0006528024315837, MSE: 1.3347706079035464e-07\n",
      "Epoch: 22, w: 1.9998826850856013, b: 1.0006061270577256, MSE: 1.1507220920062783e-07\n",
      "Epoch: 23, w: 1.9998910731019808, b: 1.0005627889730981, MSE: 9.920516118648784e-08\n",
      "Epoch: 24, w: 1.9998988613751891, b: 1.0005225495615218, MSE: 8.552598472212528e-08\n",
      "Epoch: 25, w: 1.9999060927868635, b: 1.000485187267873, MSE: 7.373299912259612e-08\n",
      "Epoch: 26, w: 1.9999128071526027, b: 1.0004504963782201, MSE: 6.356612177292104e-08\n",
      "Epoch: 27, w: 1.999919041441192, b: 1.0004182858871773, MSE: 5.480113226562651e-08\n",
      "Epoch: 28, w: 1.9999248299781465, b: 1.0003883784462437, MSE: 4.724472743962928e-08\n",
      "Epoch: 29, w: 1.999930204634709, b: 1.0003606093873374, MSE: 4.0730258273650674e-08\n",
      "Epoch: 30, w: 1.9999351950033275, b: 1.0003348258161426, MSE: 3.5114054603523776e-08\n",
      "Epoch: 31, w: 1.9999398285605896, b: 1.0003108857702883, MSE: 3.027225662089088e-08\n",
      "Epoch: 32, w: 1.9999441308185073, b: 1.0002886574377123, MSE: 2.6098083267860524e-08\n",
      "Epoch: 33, w: 1.9999481254649842, b: 1.000268018430916, MSE: 2.249947728692024e-08\n",
      "Epoch: 34, w: 1.9999518344942373, b: 1.0002488551131052, MSE: 1.9397074987164148e-08\n",
      "Epoch: 35, w: 1.9999552783278993, b: 1.0002310619725183, MSE: 1.6722455960630397e-08\n",
      "Epoch: 36, w: 1.9999584759274547, b: 1.0002145410414836, MSE: 1.4416634133877045e-08\n",
      "Epoch: 37, w: 1.999961444898642, b: 1.0001992013570178, MSE: 1.2428756890811695e-08\n",
      "Epoch: 38, w: 1.9999642015883887, b: 1.000184958459991, MSE: 1.0714983567619666e-08\n",
      "Epoch: 39, w: 1.9999667611748193, b: 1.0001717339301017, MSE: 9.23751859239906e-09\n",
      "Epoch: 40, w: 1.9999691377508197, b: 1.0001594549540993, MSE: 7.96377793809875e-09\n",
      "Epoch: 41, w: 1.999971344401636, b: 1.000148053924881, MSE: 6.865670516654653e-09\n",
      "Epoch: 42, w: 1.9999733932769188, b: 1.0001374680692519, MSE: 5.918978656762663e-09\n",
      "Epoch: 43, w: 1.9999752956576193, b: 1.0001276391023006, MSE: 5.102824007620031e-09\n",
      "Epoch: 44, w: 1.9999770620180992, b: 1.0001185129064862, MSE: 4.3992070864462876e-09\n",
      "Epoch: 45, w: 1.9999787020838051, b: 1.0001100392336726, MSE: 3.792610319555137e-09\n",
      "Epoch: 46, w: 1.9999802248848129, b: 1.000102171428465, MSE: 3.2696558159299236e-09\n",
      "Epoch: 47, w: 1.999981638805549, b: 1.0000948661713298, MSE: 2.8188103322346455e-09\n",
      "Epoch: 48, w: 1.9999829516309524, b: 1.0000880832400798, MSE: 2.430130917875329e-09\n",
      "Epoch: 49, w: 1.9999841705893395, b: 1.000081785288414, MSE: 2.095045633426433e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(1.9999841705893395), np.float64(1.000081785288414))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd(X, y, lr=0.1, epochs=50):\n",
    "    w, b = 0, 0\n",
    "    N = len(X)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        for i in range(N):\n",
    "            dw, db = gradients(X[i], y[i], w, b)\n",
    "            # 每次得到一个样本就更新一次参数\n",
    "            w-=lr*dw\n",
    "            b-=lr*db\n",
    "        mse = mse_loss(X, y, w, b)\n",
    "            \n",
    "        print(f\"Epoch: {ep}, w: {w}, b: {b}, MSE: {mse}\")\n",
    "    return w, b\n",
    "\n",
    "sgd(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c53d56",
   "metadata": {},
   "source": [
    "## 2. Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cea70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prod_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
